from time import time

start = time()

def elapsed(text):
    t = time();
    print(f"time: {t - start} - {text}")

from transformers import set_seed, GenerationConfig, GPT2LMHeadModel, GPT2Tokenizer, pipeline
import torch

elapsed("imported dependencies")

# for reproducability
SEED = 34
set_seed(SEED)

# get large GPT2 tokenizer and GPT2 model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-large", device=torch.device("mps"))

elapsed("Loaded Tokenizer")

model = GPT2LMHeadModel.from_pretrained("gpt2-large")

elapsed("Loaded Model")

#model_config.top_k=16

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
generation_config = GenerationConfig.from_pretrained("gpt2")
generation_config.pad_token_id = generation_config.eos_token_id
generation_config.max_new_tokens = 128
generation_config.tok_k = 16
generation_config.early_stopping = True
generation_config.no_repeat_ngram_size = 1
generation_config.return_dict_in_generate = True

elapsed("generated config")



sentence = [
    'You said: "Nice to meet you. What\'s your name?"\n'
    'I said: "My name is Pete."\n'
    'You said: "That\'s an interesting name. How old are you?"\n'
    'I said: "I\'m 40 years old."\n'
    'You said: "Can you tell me something about yourself?"\n'
    'I said: "Of course! I like playing video games and eating cake. "\n'
    'You said: "I like sweet stuff too. What are your plans for tomorrow?"\n'
    'I said: '
    ]
input_ids = tokenizer(sentence, return_tensors='pt').input_ids

elapsed("Tokenized inputs")


outputs = model.generate(input_ids, generation_config=generation_config)
elapsed("Model generated outputs")

# Get length of input so we can skip past it in the output
input_length = input_ids.shape[1]

# Just get the text generated by the model that comes after the input
output_tokens = outputs.sequences[:, input_length:]

output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
elapsed("Tokenized outputs")

print(output)
