from time import time

start = time()

def elapsed(text):
    t = time();
    print(f"time: {t - start} - {text}")

from transformers import set_seed, AutoTokenizer, AutoModelForCausalLM, GenerationConfig, GPT2LMHeadModel, GPT2Tokenizer, pipeline
import torch

elapsed("imported dependencies")

# for reproducability
SEED = 34
set_seed(SEED)

model_name = "EleutherAI/gpt-neo-2.7B"


# get large GPT2 tokenizer and GPT2 model
tokenizer = AutoTokenizer.from_pretrained(model_name, device=torch.device("mps"))

elapsed("Loaded Tokenizer")


model = AutoModelForCausalLM.from_pretrained(model_name)

elapsed("Loaded Model")

#model_config.top_k=16

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
generation_config = GenerationConfig.from_model_config(model.config)
generation_config.pad_token_id = generation_config.eos_token_id
generation_config.max_new_tokens = 128
#generation_config.tok_k = 16
generation_config.early_stopping = True
generation_config.no_repeat_ngram_size = 1
generation_config.return_dict_in_generate = True
generation_config.top_p = 0.9
generation_config.temperature = 0.7

elapsed("generated config")



sentence = [
    "I am a mean friend who is always sarcastic and rude.\n"
    "\n"
    'You: "Nice to meet you. What\'s your name?"\n'
    'Me: "My name is Pete."\n'
    'You: "That\'s an interesting name. How old are you?"\n'
    'Me: "I\'m 40 years old."\n'
    'You: "Can you tell me something about yourself?"\n'
    'Me: "Of course! I like playing video games and eating cake. "\n'
    'You: "I like sweet stuff too. What are your plans for tomorrow?"\n'
    'Me: '
    ]
input_ids = tokenizer(sentence, return_tensors='pt').input_ids

elapsed("Tokenized inputs")


outputs = model.generate(input_ids, generation_config=generation_config)
elapsed("Model generated outputs")

# Get length of input so we can skip past it in the output
input_length = input_ids.shape[1]

# Just get the text generated by the model that comes after the input
output_tokens = outputs.sequences[:, input_length:]

output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
elapsed("Tokenized outputs")

print(output)
